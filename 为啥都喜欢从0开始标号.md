#### 数组

数组array是一种**线性表数据结构**，它用一组连续的内存空间，来存储一组具有相同类型的数据。

~~~c
# 线性表结构： 栈，数组，链表，队列
~~~

~~~c
# 非线性表： 二叉树，图等
~~~

**线性表结构具有连续的内存空间和相同的数据类型**——杀手锏特性——随机访问——但是导致插入，**删除变得比较低效**

~~~markdown
！！寻址公式  a[i]_address = base_address + i * data_type_seze
~~~

**纠正一个错误，数组和链表的区别并不是说链表的操作时间复杂度是o(1),而数组查找的时间复杂度是o(1)**

~~~markdown
实际上，这种表述是错误的。数组是适合查找操作，但是查找的时间复杂度并不为O（1）.即便是排好序的数组，你用二分查找，时间复杂度也是O（logn）。所以，正确的表述应该是，数组支持随机访问，根据下标随机访问的时间复杂度为O（1）
~~~

**再看插入操作**

​		如果数组是严格有序的，除非插入到最后的位置，时间复杂度为O（1），但是其他位置，都涉及到移动数据，时间复杂度是O（n），因为每个位置插入元素的概率是一样的，所以平均情况时间复杂度是（1+2+3+.....+n）/n,求极限，时间复杂度是O（n）



​		**但是如果数据只是被当作一个存储数据的集合**，这种情况下，如果要将某个数据插入到第k个位置，就可以直接将第k位的数据搬到数组的最后面去，把新元素放到第k个位置——**所以在这中特定的场景下，随机插入一个元素的时间复杂度会降为O（1），这种思想用在快排上很nice**



**再看删除操作**

​		和插入类似，如果删除末尾数据，时间复杂度O（1），其他时间复杂度O（n），所以平均时间复杂度O（n）。

​		**实际上，在某些特殊的场景下，我们不一定非得追求数组中数据的连续性。如果我们降多次删除操作集中在一起执行，删除的效率就提高了**

​		首先，记录下已经删除的数据。每次的删除操作并不是真正的搬移数据，**只是记录数据已经被删除。当数组没有更多的空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移**

——**其实这就是JVM的标记清楚垃圾的回收算法的核心思想**

~~~markd
很多时候我们并不是要死记硬背某个数据结构或者算法，而是要学习它背后的思想和处理技巧，这些东西才是最有家直的东西
~~~

**警惕数组的访问越界问题**

略（很多高级语言会处理掉这个问题，会自动做越界检查，在C里这种越界属于**未决行为**）



#### 回到问题：为什么都喜欢从0开始

​		**下标**的真正含义，最确切的定义应该是**偏移offset**，看下内存地址计算公式

~~~markdown
！！寻址公式  a[i]_address = base_address + i * data_type_seze
~~~

如果数组从1开始：

~~~markdown
a[i]_address = base_address + (i -1)* data_type_seze
~~~

对比两个公式，下面的多做了一次减法，对CPU来说，就是多了一次减法指令

数组作为非常基础的数据结构，通过下标随机访问数组元素又是非常基础的编程操作，**效率的优化就要尽可能的做到极致！！**



**但是，最有力原因其实应该是历史原因！！！，C从0开始定义下标的，然后java，js纷纷效仿，也可以说为了学C的可以更快的搞定java，就延用了原来的**



**实际上很多语言不从0开始计数的，我python还可以从负数，matlab也不是从0的**